{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lifeifan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#function to pre process text data\n",
    "\n",
    "# dataframe: dataframe object\n",
    "\n",
    "# target_column_name: column on which stop words removal is needed\n",
    "\n",
    "# new_column_name: new column to store the cleaned data\n",
    "\n",
    "# 1. remove stop words\n",
    "\n",
    "# 2. remove puncutations etc\n",
    "\n",
    "# 3. remove special characters\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(\"/Users/lifeifan/Desktop/ece1786/project/data1.csv\")\n",
    "\n",
    "df['cmp_code'] = df['cmp_code'].replace({'conservatism': 0, 'progressivism': 1})\n",
    "df = df.rename(columns={'cmp_code': 'label'})\n",
    "\n",
    "def remove_stop_words (dataframe,target_column_name,new_column_name) :\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x:' '.join([item for item in x.split() if item not in stopwords.words('english')]))\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def remove_punctuations(dataframe,target_column_name,new_column_name):\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
    "    return dataframe\n",
    "\n",
    "def stem_text(dataframe,target_column_name,new_column_name):\n",
    "    dataframe[new_column_name] = dataframe[target_column_name].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label  \\\n",
      "0     But too many Americans have been left out and ...      1   \n",
      "1               and the racial wealth gap remains wide,      1   \n",
      "2                empowerment is better than resentment,      1   \n",
      "3                    and bridges are better than walls.      1   \n",
      "4     —an economy that grows incomes for working peo...      1   \n",
      "...                                                 ...    ...   \n",
      "1995  To prevent sectors becoming partially or wholl...      0   \n",
      "1996     We will promote integration and British values      0   \n",
      "1997  Being able to speak English is a fundamental p...      0   \n",
      "1998  We have introduced tough new language tests fo...      0   \n",
      "1999  Next, we will legislate to ensure that every p...      0   \n",
      "\n",
      "                                           cleaned_text  \n",
      "0                  But many Americans left left behind.  \n",
      "1                       racial wealth gap remains wide,  \n",
      "2                        empowerment better resentment,  \n",
      "3                                 bridges better walls.  \n",
      "4             —an economy grows incomes working people,  \n",
      "...                                                 ...  \n",
      "1995  To prevent sectors becoming partially wholly r...  \n",
      "1996              We promote integration British values  \n",
      "1997  Being able speak English fundamental part inte...  \n",
      "1998  We introduced tough new language tests migrant...  \n",
      "1999  Next, legislate ensure every public sector wor...  \n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = remove_stop_words(df,\"text\",\"cleaned_text\")\n",
    "print(cleaned_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = cleaned_data.cleaned_text.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, val_input, train_label, val_label=train_test_split (texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(val_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  375\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for content in texts:\n",
    "    max_len = max(max_len, len(content))\n",
    "print('Max sentence length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token [2021, 2116, 4841, 2187, 2187, 2369, 1012]\n"
     ]
    }
   ],
   "source": [
    "print(\"token\",tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "def mask_inputs_for_bert (text,max_len):\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = []\n",
    "    attention_masks = []\n",
    "    input_ids = []\n",
    "    i = 0\n",
    "    # For every sentence...\n",
    "    for content in text:\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            content,\n",
    "            add_special_tokens = True, # Add '[CLS]' and '[SEP]' \n",
    "            max_length= max_len,\n",
    "            # Sentence to encode.\n",
    "            # Pad & truncate all sentences.\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True, # Construct attn. masks.\n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        # And its attention mask (simply differentiates padding from non-padding). \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        #convert to tensor and return\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_input_shape (1500, 375)\n",
      "Train_mask_shape (1500, 375)\n",
      "Validation_input_shape (500, 375)\n",
      "Validation_mask_shape (500, 375)\n",
      "Train_labelshape (1500,)\n",
      "Validation_label_shape (500,)\n"
     ]
    }
   ],
   "source": [
    "train_inp, train_mask = mask_inputs_for_bert (train_input,max_len) \n",
    "val_inp, val_mask = mask_inputs_for_bert (val_input, max_len) \n",
    "train_label =  tf.convert_to_tensor(train_label)\n",
    "val_label =  tf.convert_to_tensor(val_label)\n",
    "print(\"Train_input_shape\", train_inp.shape) \n",
    "print(\"Train_mask_shape\", train_mask.shape) \n",
    "print(\"Validation_input_shape\", val_inp.shape) \n",
    "print(\"Validation_mask_shape\", val_mask.shape) \n",
    "print(\"Train_labelshape\", train_label.shape) \n",
    "print(\"Validation_label_shape\", val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  2057  2442 ...     0     0     0]\n",
      " [  101  4895 11787 ...     0     0     0]\n",
      " [  101  2057  2036 ...     0     0     0]\n",
      " ...\n",
      " [  101  3914  3952 ...     0     0     0]\n",
      " [  101  8037  2903 ...     0     0     0]\n",
      " [  101  7659  2610 ...     0     0     0]], shape=(1500, 375), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_dir='/Users/lifeifan/Desktop/ece1786/project/'\n",
    "model_save_path='/Users/lifeifan/Desktop/ece1786/project/bert.h5'\n",
    "callbacks = [tf.keras.callbacks. ModelCheckpoint (filepath=model_save_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    monitor= 'val_loss', \n",
    "                                                    mode='min',\n",
    "                                                    save_best_only=True),\n",
    "                                                    tf.keras.callbacks.TensorBoard (log_dir=log_dir)]\n",
    "print('\\nBert Model', model.summary())\n",
    "loss = tf.keras.losses. SparseCategoricalCrossentropy (from_logits=True) \n",
    "metric = tf.keras.metrics. SparseCategoricalAccuracy('accuracy')\n",
    "optimizer= tf.keras.optimizers.legacy.Adam (learning_rate=2e-5, epsilon=1e-08)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2097s 22s/step - loss: 0.6869 - accuracy: 0.5387 - val_loss: 0.6042 - val_accuracy: 0.6820\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([train_inp, train_mask],\n",
    "                    train_label,\n",
    "                    batch_size=32,\n",
    "                    epochs=8,\n",
    "                    validation_data=([val_inp, val_mask], val_label),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(history[\u001b[39m'\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(history[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation accuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast,TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=2)\n",
    "model.load_weights('/Users/lifeifan/Desktop/ece1786/project/bert.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score\n\u001b[0;32m----> 2\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict([val_inp,val_mask],batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_inp' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds = model.predict([val_inp,val_mask],batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.6940639269406392\n",
      "Classification Report\n",
      "Training and saving built model.....\n"
     ]
    }
   ],
   "source": [
    "pred_labels = preds.logits.argmax(axis=1)\n",
    "f1 = f1_score(val_label,pred_labels)\n",
    "print('F1 score',f1)\n",
    "print('Classification Report')\n",
    "# print(classification_report(val_label,pred_labels,target_names=target_names))\n",
    "\n",
    "print('Training and saving built model.....') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.21499395 0.785006  ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess your single sentence\n",
    "input_text = \"The working class can surely win.\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "predictions = model(input_ids)\n",
    "import numpy as np\n",
    "\n",
    "logits = predictions.logits\n",
    "probabilities = tf.nn.softmax(logits, axis=1)\n",
    "print(probabilities)\n",
    "predicted_class = tf.argmax(probabilities, axis=1)\n",
    "print(predicted_class)\n",
    "\n",
    "class_labels = ['con', 'pro']\n",
    "predicted_label = class_labels[predicted_class.numpy()[0]]\n",
    "print(predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
